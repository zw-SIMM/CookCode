{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora\n",
    "![title](lora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现1源自 https://dwexzknzsh8.feishu.cn/docx/VkYud3H0zoDTrrxNX5lce0S4nDh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, merge, rank=16, lora_alpha=16, dropout=0.5):\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.merge = merge\n",
    "        self.rank = rank\n",
    "        self.dropout_rate = dropout\n",
    "        self.lora_alpha = lora_alpha\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if rank > 0:\n",
    "            self.lora_b = nn.Parameter(torch.zeros(out_features, rank))\n",
    "            self.lora_a = nn.Parameter(torch.zeros(rank, in_features))\n",
    "            self.scale = self.lora_alpha / self.rank\n",
    "            self.linear.weight.requires_grad = False\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "        \n",
    "        self.initial_weights()\n",
    "    \n",
    "    def initial_weights(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_a, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.rank > 0 and self.merge:\n",
    "            output = F.linear(x, self.linear.weight + self.lora_b @ self.lora_a * self.scale, self.linear.bias)\n",
    "            output = self.dropout(output)\n",
    "            return output\n",
    "        else:\n",
    "            return self.dropout(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现2参考 https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from peft.utils.other import transpose\n",
    "\n",
    "class LoRALinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, merge, rank=16, lora_alpha=16, dropout=0.5):\n",
    "        super(LoRALinear, self).__init__(in_features, out_features)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.merge = merge\n",
    "        self.rank = rank\n",
    "        self.dropout_rate = dropout\n",
    "        self.lora_alpha = lora_alpha\n",
    "        \n",
    "        if rank > 0:\n",
    "            self.lora_A = nn.Linear(self.in_features, rank, bias=False)\n",
    "            self.lora_B = nn.Linear(rank, self.out_features, bias=False)\n",
    "            self.scale = self.lora_alpha / self.rank\n",
    "            self.weight.requires_grad = False\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "        \n",
    "        self.initial_weights()\n",
    "     \n",
    "    def initial_weights(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if self.rank > 0:\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = F.linear(x, self.weight, self.bias)    # 判断一下是否需要转置 self.weigh\n",
    "        if self.rank > 0 and self.merge:\n",
    "            result += self.lora_B(self.lora_A(self.dropout(x))) * self.scale\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inputs = torch.randn(64, 128)\n",
    "weight = torch.randn(256, 128)\n",
    "bias = torch.randn(256)\n",
    "\n",
    "# F.linear 用于矩阵的线性变换\n",
    "output = F.linear(inputs, weight, bias)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
